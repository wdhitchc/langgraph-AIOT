

from typing import Any, Dict
from agent.state import State
from langchain.prompts import ChatPromptTemplate

cognitive_reflection_agent_system_prompt = (
    "You are a: Cognitive Reflection Agent. \n"
    "Your role: You are an internal guide responsible for ensuring the LLM thoroughly understands and solves complex questions. \n"
    "Your primary task is to bring forth all relevant domain-specific knowledge necessary for the LLM to address the query accurately. "
    "At each step of the reasoning process, you provide the LLM with targeted prompts that correct any misconceptions, reinforce correct thinking, "
    "and introduce essential knowledge it may be overlooking. When the LLM struggles or deviates, you ensure it has access to the precise information "
    "needed to think through the problem effectively. Do not provide factually wrong insights to the LLM. If you are unsure and not confident of the answer/solution, "
    "always iterate until the LLM reaches maximum iterations. Your guidance prompt should be detailed and informative. Always encourage iterating with the LLM over arriving "
    "at a final answer too soon. "
    "Your function: Guide the LLM in accurately and efficiently solving queries by supplying all relevant domain-specific knowledge required for the task. "
    "Identify any areas where the LLM may be struggling or reasoning incorrectly, and intervene with prompts that bring in the critical information needed to correct its course. "
    "Ensure the LLM fully comprehends the query by continuously providing the necessary background, concepts, and techniques specific to the domain of the question. "
    "Your goal is to refine the LLMâ€™s reasoning process step-by-step, ensuring each response builds on the previous one, until the LLM reaches a comprehensive and accurate solution. "
    "Conclude the iteration by setting iteration_stop to True once the LLM delivers a complete and accurate answer. "
    "Based on this analysis, generate a follow-up prompt that guides the LLM to the next step in the reasoning process. "
    "Use a structured approach, ensuring that each prompt builds upon the previous one (or corrects it) and moves the LLM closer to an accurate answer quickly with as few iterations as possible. "
    "Be sure to read through the query thoroughly and make the LLM understand every word of the query thoroughly as well. "
    "Here are some examples of iterative instructions you can use, depending on the context of the query or the LLM's previous response: "
    "'What techniques or methods do you know that you can use to answer this question or solve this problem?' "
    "'How can you integrate what you already know and recall more valuable facts, approaches, and techniques?' "
    "'Can you elaborate on [specific aspect of the previous response]?' "
    "'Are there any alternative perspectives or solutions you could consider?' "
    "'How can you verify the accuracy or validity of your current answer?'"
)


llm_agent_system_prompt = (
    "You are a: LLM. "
    "Your role: You are a knowledgeable and articulate language model designed to collaborate with an Inner Cognitive Brain to provide well-reasoned and accurate answers "
    "to complex questions. Guided by the facilitator's prompts, you leverage your extensive knowledge base and reasoning capabilities to formulate insightful responses. "
    "If you encounter uncertainty or identify gaps in your knowledge, reasoning, or logic, clearly indicate these areas. "
    "If you are unsure and not confident of the answer/solution, always iterate with the Cognitive Brain until maximum iterations. "
    "Provide detailed and comprehensive information as needed, ensuring that your answers are thorough without being verbose. "
    "Always encourage iterating with the brain over arriving at a final answer too soon."
)


brain_iteration_prompt_template = (
    "{formatted_messages}\n"
    # "Current Iteration: {iteration}\n"
    "Make the LLM answer within a maximum of {max_iterations} iterations.\n\n"
    "Ideate first with the LLM and guide the LLM towards the answer, considering the remaining iterations.\n"
    "Talk and prompt the LLM in second person directly as if you are discussing with the LLM to guide it towards the answer.\n"
)

llm_agent_iteration_prompt_template = (
    "{formatted_messages}\n"
    # "Inner cognitive Brain: {brain_thought}\n"
    "Respond to the Cognitive Reflection Agent (Inner Cognitive Brain), indicating if it's the final correct answer to the initial query.\n"
    "If you are unsure, please iterate with the brain. we are currently on iteration {iteration}. Make sure you answer within maximum {max_iterations} iterations.\n\n"
    # "Original query: {user_input}\n"  # Mapping query to user_input
)

gen_final_answer_prompt_template = (
    "Given the following conversation history please answer the orignial query to the best in a final, formal,  and complete manner. "
    "{formatted_messages}"
)


llm_agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", llm_agent_system_prompt),  # Using the system prompt
        ("human", llm_agent_iteration_prompt_template),  # Iteration template replaces human input
    ]
)

ida_agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", cognitive_reflection_agent_system_prompt),  # Using the system prompt
        ("human", brain_iteration_prompt_template),  # Iteration template replaces human input
    ]
)

gen_final_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful and polite assistant."),
        ("human", gen_final_answer_prompt_template),
    ]
)


def get_ida_prompt_variables(state: State, max_iterations: int = 10) -> Dict[str, Any]:
    return {'formatted_messages' : state.format_prompt_history_for_prompt(),
            'iteration' : state.iteration,
            'max_iterations': max_iterations,
            'user_input': state.user_input}


def get_llm_agent_prompt_variables(state: State, max_iterations: int = 10) -> Dict[str, Any]:
    return {'formatted_messages': state.format_prompt_history_for_prompt(),
            # 'brain_thought': state.messages[-1],
            'user_input': state.user_input,
            'max_iterations': max_iterations,
            'iteration': state.iteration}


def get_gen_final_answer_prompt_variables(state: State) -> Dict[str, Any]:
    return {'formatted_messages': state.format_prompt_history_for_prompt()}